<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Canal介绍与使用]]></title>
    <url>%2Fblog%2F2023%2F03%2F15%2FCanal%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Canal介绍与使用简介 canal [kə’næl]，译意为水道/管道/沟渠，主要用途是基于 MySQL 数据库增量日志解析，提供增量数据订阅和消费 早期阿里巴巴因为杭州和美国双机房部署，存在跨机房同步的业务需求，实现方式主要是基于业务 trigger 获取增量变更。从 2010 年开始，业务逐步尝试数据库日志解析获取增量变更进行同步，由此衍生出了大量的数据库增量订阅和消费业务。 基于日志增量订阅和消费的业务包括 数据库镜像 数据库实时备份 索引构建和实时维护(拆分异构索引、倒排索引等) 业务 cache 刷新 带业务逻辑的增量数据处理 当前的 canal 支持源端 MySQL 版本包括 5.1.x , 5.5.x , 5.6.x , 5.7.x , 8.0.x 注意：本文中使用的canal版本为1.1.7版本 更多详细内容可以参考Home · alibaba/canal Wiki (github.com) 工作原理MySQL主备复制原理 MySQL master 将数据变更写入二进制日志( binary log, 其中记录叫做二进制日志事件binary log events，可以通过 show binlog events 进行查看) MySQL slave 将 master 的 binary log events 拷贝到它的中继日志(relay log) MySQL slave 重放 relay log 中事件，将数据变更反映它自己的数据 canal 工作原理 canal 模拟 MySQL slave 的交互协议，伪装自己为 MySQL slave ，向 MySQL master 发送dump 协议 MySQL master 收到 dump 请求，开始推送 binary log 给 slave (即 canal ) canal 解析 binary log 对象(原始为 byte 流) 架构与概念 canal是一个CS架构的程序，因此分为client与server,其中server可以包含多个instance server：代表一个canal运行实例，对应于一个jvm，本质是一个基于SpringBoot的Web项目（包含Netty和Embeded两种实现），可以通过接口启动、配置、调度、监控instance instance：对应于一个数据队列，这个数据队列会完成解析binlog日志、binlog日志过滤、 binlog日志转储、位点元数据管理等核心功能,instance包含以下四个核心组件： eventParser :数据源接入，模拟slave协议和master进行交互，协议解析 eventSink :Parser和Store链接器，进行数据过滤，加工，分发的工作 eventStore :数据存储，目前仅实现了Memory内存模式 metaManager :增量订阅&amp;消费信息管理器 client：消费数据的客户端，可以自己参考官方demo写java实现，也可以直接使用官方提供的工具包（CilentAdapter） 相关组件 canal.deployer:canal主体程序 canal.admin:canal 的Web版管理后台，可以通过图形化界面管理配置参数,从而动态启停 Server 和 Instance，本文对此不多做深入研究 canal.adapter:应当称为ClientAdapter,是在canal 1.1.1版本之后, 增加的客户端数据落地的适配及启动功能，目前支持功能: 客户端启动器 同步管理REST接口 日志适配器, 作为DEMO 关系型数据库的数据同步(表对表同步), ETL功能 HBase的数据同步(表对表同步), ETL功能 ElasticSearch多表数据同步,ETL功能 另外还有集群HA的解决方案，但是目前暂时够用未深入研究 一般来说，用一个server带起若干instance，再加上adapter已经足够普通用户的使用 Server-QuickStart准备 纯java开发，windows/linux均可支持 jdk建议使用1.6.25以上的版本 对于自建 MySQL , 需要先开启 Binlog 写入功能，配置 binlog-format 为 ROW 模式，my.cnf 中配置如下 1234[mysqld]log-bin=mysql-bin # 开启 binlogbinlog-format=ROW # 选择 ROW 模式server_id=1 # 配置 MySQL replaction 需要定义，不要和 canal 的 slaveId 重复 注意：针对阿里云 RDS for MySQL , 默认打开了 binlog , 并且账号默认具有 binlog dump 权限 , 不需要任何权限或者 binlog 设置,可以直接跳过这一步 授权 canal 链接 MySQL 账号具有作为 MySQL slave 的权限, 如果已有账户可直接 grant 1234CREATE USER canal IDENTIFIED BY 'canal'; GRANT SELECT, REPLICATION SLAVE, REPLICATION CLIENT ON *.* TO 'canal'@'%';-- GRANT ALL PRIVILEGES ON *.* TO 'canal'@'%' ;FLUSH PRIVILEGES; 启动 下载 canal, 访问 release 页面 , 选择需要的包下载, 如以 1.0.17 版本为例 1wget https://github.com/alibaba/canal/releases/download/canal-1.0.17/canal.deployer-1.0.17.tar.gz 解压缩 12mkdir /tmp/canaltar zxvf canal.deployer-$version.tar.gz -C /tmp/canal 解压完成后，进入 /tmp/canal 目录，可以看到如下结构 1234drwxr-xr-x 2 jianghang jianghang 136 2013-02-05 21:51 bindrwxr-xr-x 4 jianghang jianghang 160 2013-02-05 21:51 confdrwxr-xr-x 2 jianghang jianghang 1.3K 2013-02-05 21:51 libdrwxr-xr-x 2 jianghang jianghang 48 2013-02-05 21:29 logs 配置修改 1vi conf/example/instance.properties 123456789101112131415161718## mysql serverIdcanal.instance.mysql.slaveId = 1234#position info，需要改成自己的数据库信息canal.instance.master.address = 127.0.0.1:3306 canal.instance.master.journal.name = canal.instance.master.position = canal.instance.master.timestamp = #canal.instance.standby.address = #canal.instance.standby.journal.name =#canal.instance.standby.position = #canal.instance.standby.timestamp = #username/password，需要改成自己的数据库信息canal.instance.dbUsername = canal canal.instance.dbPassword = canalcanal.instance.defaultDatabaseName =canal.instance.connectionCharset = UTF-8#table regexcanal.instance.filter.regex = .\*\\\\..\* canal.instance.connectionCharset 代表数据库的编码方式对应到 java 中的编码类型，比如 UTF-8，GBK , ISO-8859-1 如果系统是1个 cpu，需要将 canal.instance.parser.parallel 设置为 false 启动 1sh bin/startup.sh 查看 server 日志 1vi logs/canal/canal.log&lt;/pre&gt; 1232013-02-05 22:45:27.967 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## start the canal server.2013-02-05 22:45:28.113 [main] INFO com.alibaba.otter.canal.deployer.CanalController - ## start the canal server[10.1.29.120:11111]2013-02-05 22:45:28.210 [main] INFO com.alibaba.otter.canal.deployer.CanalLauncher - ## the canal server is running now ...... 查看 instance 的日志 1vi logs/example/example.log 12342013-02-05 22:50:45.636 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [canal.properties]2013-02-05 22:50:45.641 [main] INFO c.a.o.c.i.spring.support.PropertyPlaceholderConfigurer - Loading properties file from class path resource [example/instance.properties]2013-02-05 22:50:45.803 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start CannalInstance for 1-example 2013-02-05 22:50:45.810 [main] INFO c.a.otter.canal.instance.spring.CanalInstanceWithSpring - start successful.... 关闭 1sh bin/stop.sh 踩坑 base table不存在报错：canal.deployer-1.1.6版本的安装包有bug #4245](https://github.com/alibaba/canal/issues/4245)) 问题原因： 为了增加新功能而导致的源码包与发布包不一致，且发布包有问题，具体信息可参考下面的链接， 兼容PolarDB-X的show databases返回 · Issue #4216 · alibaba/canal (github.com) 解决办法： 自己重新下载源码重新打包depolyer，源码中已将此问题修复，但是发布的包并没有 在instance.properties中应用如下设置过滤掉所有schema中的BASE TABLE表 1canal.instance.filter.black.regex=.*\\.BASE TABLE.* 在instance.properties中应用如下设置只同步特定的表从而忽略BASE TABLE 1canal.instance.filter.regex=aglaia.tb_member,aglaia.tb_member_card Adapter整体结构client-adapter分为适配器和启动器两部分, 适配器为多个fat jar, 每个适配器会将自己所需的依赖打成一个包, 以SPI的方式让启动器动态加载, 目前所有支持的适配器都放置在plugin目录下 启动器为 SpringBoot 项目, 支持canal-client启动的同时提供相关REST管理接口, 运行目录结构为: 12345678910111213141516- bin restart.sh startup.bat startup.sh stop.sh- lib ...- plugin client-adapter.logger-1.1.1-jar-with-dependencies.jar client-adapter.hbase-1.1.1-jar-with-dependencies.jar ...- conf application.yml - hbase mytest_person2.yml- logs 配置介绍有两种方式可以对整个adapter进行配置，一种是使用总配置文件 application.yml，另一种则是使用bootstrap.yml将配置记录在数据库中 总配置文件 application.yml12345678910111213141516171819202122canal.conf: canalServerHost: 127.0.0.1:11111 # 对应单机模式下的canal server的ip:port zookeeperHosts: slave1:2181 # 对应集群模式下的zk地址, 如果配置了canalServerHost, 则以canalServerHost为准 mqServers: slave1:6667 #or rocketmq # kafka或rocketMQ地址, 与canalServerHost不能并存 flatMessage: true # 扁平message开关, 是否以json字符串形式投递数据, 仅在kafka/rocketMQ模式下有效 batchSize: 50 # 每次获取数据的批大小, 单位为K syncBatchSize: 1000 # 每次同步的批数量 retries: 0 # 重试次数, -1为无限重试 timeout: # 同步超时时间, 单位毫秒 mode: tcp # kafka rocketMQ # canal client的模式: tcp kafka rocketMQ srcDataSources: # 源数据库 defaultDS: # 自定义名称 url: jdbc:mysql://127.0.0.1:3306/mytest?useUnicode=true # jdbc url username: root # jdbc 账号 password: 121212 # jdbc 密码 canalAdapters: # 适配器列表 - instance: example # canal 实例名或者 MQ topic 名 groups: # 分组列表 - groupId: g1 # 分组id, 如果是MQ模式将用到该值 outerAdapters: # 分组内适配器列表 - name: logger # 日志打印适配器...... 说明: 一份数据可以被多个group同时消费, 多个group之间会是一个并行执行, 一个group内部是一个串行执行多个outerAdapters, 比如例子中logger和hbase 目前client adapter数据订阅的方式支持两种，直连canal server 或者 订阅kafka/RocketMQ的消息 使用远程配置(Mysql) 创建mysql schema 1CREATE SCHEMA `canal_manager` DEFAULT CHARACTER SET utf8mb4 ; 初始化数据 使用canal_manager.sql脚本建表并初始化Demo数据，其中canal_config表id=2的数据对应adapter下的application.yml文件，canal_adapter_config表对应每个adapter的子配置文件 修改bootstrap.yml配置 123456canal: manager: jdbc: url: jdbc:mysql://127.0.0.1:3306/canal_manager?useUnicode=true&amp;characterEncoding=UTF-8 username: root password: 121212 可以将本地application.yml文件和其他子配置文件删除或清空， 启动工程将自动从远程加载配置 修改mysql中的配置信息后会自动刷新到本地动态加载相应的实例或者应用 QuickStart 启动canal server 修改conf/application.yml为: 123456789101112131415161718192021222324server: port: 8081logging: level: com.alibaba.otter.canal.client.adapter.hbase: DEBUGspring: jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_nullcanal.conf: canalServerHost: 127.0.0.1:11111 batchSize: 500 syncBatchSize: 1000 retries: 0 timeout: mode: tcp canalAdapters: - instance: example groups: - groupId: g1 outerAdapters: - name: logger 启动 1bin/startup.sh adapter管理REST接口 查询所有订阅同步的canal instance或MQ topic 1curl http://127.0.0.1:8081/destinations 数据同步开关 1curl http://127.0.0.1:8081/syncSwitch/example/off -X PUT 针对 example 这个canal instance/MQ topic 进行开关操作. off代表关闭, instance/topic下的同步将阻塞或者断开连接不再接收数据, on代表开启 注: 如果在配置文件中配置了 zookeeperHosts 项, 则会使用分布式锁来控制HA中的数据同步开关, 如果是单机模式则使用本地锁来控制开关 数据同步开关状态 1curl http://127.0.0.1:8081/syncSwitch/example 查看指定 canal instance/MQ topic 的数据同步开关状态 手动ETL 1curl http://127.0.0.1:8081/etl/hbase/mytest_person2.yml -X POST -d "params=2018-10-21 00:00:00" 导入数据到指定类型的库, 如果params参数为空则全表导入, 参数对应的查询条件在配置中的etlCondition指定 查看相关库总数据 1curl http://127.0.0.1:8081/count/hbase/mytest_person2.yml 适配器列表logger适配器1234最简单的处理, 将受到的变更事件通过日志打印的方式进行输出, 如配置所示, 只需要定义name: logger即可... outerAdapters: - name: logger Hbase适配器同步HBase配置 : Sync-HBase RDB适配器同步关系型数据库配置 : Sync-RDB 目前内置支持的数据库列表: MySQL Oracle PostgresSQL SQLServer 使用了JDBC driver,理论上支持绝大部分的关系型数据库 ES适配器同步关ES配置 : Sync-ES MongoDB适配器Redis适配器RDB适配器与PostgreSQL配合使用操作步骤 修改启动器配置conf/application.yml 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667server: port: 8081spring: jackson: date-format: yyyy-MM-dd HH:mm:ss time-zone: GMT+8 default-property-inclusion: non_nullcanal.conf: mode: tcp #tcp kafka rocketMQ rabbitMQ flatMessage: true zookeeperHosts: syncBatchSize: 1000 retries: 3 timeout: 10 accessKey: secretKey: consumerProperties: # canal tcp consumer canal.tcp.server.host: 127.0.0.1:11111 canal.tcp.zookeeper.hosts: canal.tcp.batch.size: 500 canal.tcp.username: canal.tcp.password: # kafka consumer kafka.bootstrap.servers: 127.0.0.1:9092 kafka.enable.auto.commit: false kafka.auto.commit.interval.ms: 1000 kafka.auto.offset.reset: latest kafka.request.timeout.ms: 40000 kafka.session.timeout.ms: 30000 kafka.isolation.level: read_committed kafka.max.poll.records: 1000 # rocketMQ consumer rocketmq.namespace: rocketmq.namesrv.addr: 127.0.0.1:9876 rocketmq.batch.size: 1000 rocketmq.enable.message.trace: false rocketmq.customized.trace.topic: rocketmq.access.channel: rocketmq.subscribe.filter: # rabbitMQ consumer rabbitmq.host: rabbitmq.virtual.host: rabbitmq.username: rabbitmq.password: rabbitmq.resource.ownerId: srcDataSources: #导出数据的mysql数据源配置，后续会在表映射配置文件中使用到 defaultDS: url: jdbc:mysql://127.0.0.1:3306/mytest?useUnicode=true username: root password: 121212 canalAdapters: - instance: example # canal instance Name or mq topic name groups: - groupId: g1 outerAdapters: - name: logger - name: rdb # 指定为rdb类型同步 key: postgres1 # 指定adapter的唯一key, 与表映射配置文件中outerAdapterKey对应 properties: #目标数据库的链接信息 jdbc.driverClassName: org.postgresql.Driver # jdbc驱动名, 部分jdbc的jar包需要自行放致lib目录下 jdbc.url: jdbc:postgresql://localhost:5432/postgres # jdbc url jdbc.username: postgres # jdbc username jdbc.password: 121212 # jdbc password threads: 1 # 并行执行的线程数, 默认为1 commitSize: 3000 修改 conf/rdb/mytest_user.yml文件 1234567891011121314151617181920dataSourceKey: defaultDS # 源数据源的key, 对应上面配置的srcDataSources中的值destination: example # cannal的instance或者MQ的topicgroupId: g1 # 对应MQ模式下的groupId, 只会同步对应groupId的数据outerAdapterKey: postgres1 # adapter key, 对应上面配置outAdapters中的keyconcurrent: true # 是否按主键hash并行同步, 并行同步的表必须保证主键不会更改及主键不能为其他同步表的外键!!dbMapping: database: mytest # 源数据源的database/shcema table: user # 源数据源表名 targetTable: ods.tb_user # 目标数据源的模式名.表名 targetPk: # 主键映射 id: id # 如果是复合主键可以换行映射多个 #mapAll: true # 是否整表映射, 要求源表和目标表字段名一模一样 (如果targetColumns也配置了映射,则以targetColumns配置为准) targetColumns: # 字段映射, 格式: 目标表字段: 源表字段, 如果字段名一样源表字段名可不填 id: name: role_id: c_time: test1: #etlCondition: "where c_time&gt;=&#123;&#125;" commitBatch: 3000 # 批量提交的大小 启动RDB程序 将目标库的jdbc jar包放入lib文件夹, 这里放入ojdbc6.jar (如果是其他数据库则放入对应的驱动) 启动canal-adapter启动器 1bin/startup.sh 全量同步一遍数据 1curl http://127.0.0.1:8081/etl/rdb/mytest_user.yml -X POST 确认RDB是否增长运行 1curl http://127.0.0.1:8081/destinations 踩坑 全量同步报错： 12ERROR c.a.otter.canal.client.adapter.rdb.service.RdbEtlService - org.postgresql.util.PSQLException: Fetch size must be a value greater to or equal to 0.java.lang.RuntimeException: org.postgresql.util.PSQLException: Fetch size must be a value greater to or equal to 0. 问题原因： 错误设置postgres数据库的FetchSize postgres全量同步报错 · Issue #2146 · alibaba/canal (github.com) 解决办法： 修改client-adapter/common/src/main/java/com/alibaba/otter/canal/client/adapter/support/Util.java文件第41行和第55行，如下图所示 此问题目前只能通过此方式解决，需要自己自己修改源码后重新打包 同步报错：某字段 not matched 问题原因： sql关键字错误，误认为所有数据库的表名都可以用mysql下列形式表示，导致字段全都无法匹配 1`dbname`.`tablename` adapter 同步到目标库 报错Target column: id not matched · Issue #2941 · alibaba/canal (github.com) 经调查，根据issue内的方法直接改client-adapter/rdb/src/main/java/com/alibaba/otter/canal/client/adapter/rdb/support/SyncUtil.java中的getDBTableName和getBacktickByDbType方法是无用的，因为该方法其实并没有错误如下图 需要修改的是client-adapter/rdb/src/main/java/com/alibaba/otter/canal/client/adapter/rdb/service/RdbEtlService.java中的executeSqlImport方法，此方法中错误的将需要使用目标数据源的地方用了源头数据源，导致了关键字全部按照mysql的方法进行拼接，最终导致错误，因此需要做如下修改： 做完以上修改后，重新运行如下打包命令,然后在项目根目录的target文件夹下找到对应的包重新部署： 1mvn clean install -Dmaven.test.skip -Denv=release 全量同步报错： 12 ERROR c.a.otter.canal.client.adapter.rdb.service.RdbEtlService - com.alibaba.druid.pool.GetConnectionTimeoutException: wait millis 60000, active 3, maxActive 3, creating 0java.lang.RuntimeException: com.alibaba.druid.pool.GetConnectionTimeoutException: wait millis 60000, active 3, maxActive 3, creating 0 问题原因：默认连接池最大值设置只有三个，数据量大，线程多的情况下就会报错解决办法：修改application.yml中srcDataSources下对应dataSource的最大连接数，如下图： 1234567srcDataSources: defaultDS: url: 这里填源数据库的jdbc连接信息，例：jdbc:mysql://127.0.0.1:3306/testdb username: 数据库账号，例：root password: 数据库密码，例：root maxActive: 100 #额外增加这一行，默认的连接数只有3，会导致全量同步出现异常，导致全量同步数据缺失，最好改大一点canalAdapters:]]></content>
      <categories>
        <category>数据同步</category>
      </categories>
      <tags>
        <tag>canal</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Docker Swarm的介绍与使用]]></title>
    <url>%2Fblog%2F2022%2F12%2F18%2FDocker-Swarm%E7%9A%84%E4%BB%8B%E7%BB%8D%E4%B8%8E%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[Docker Swarm的介绍与使用What Docker(容器) Swarm(集群)——简单来说，Docker swarm就是容器集群（包含了集群管理和编排功能），由Docker公司研发并且在Dokcer v1.12版本后自带此服务（早期作为一项独立服务叫做swarmkit，需要单独安装）。 集群管理：创建新集群、升级集群的主节点和工作节点、执行节点维护(例如内核升级)和升级运行集群的版本等。 容器编排：应用一般由单独容器化的组件（通常称为微服务）组成，这些组件必须按特定顺序在网络中进行组织并照计划运行，这种对多个容器进行组织的流程即称为容器编排（使用Docker Compose实际就是容器编排）。 For Example: standalone-mysql-5.7.yaml 1234567891011121314151617181920212223242526272829303132333435363738394041version: "2"services: nacos: # 服务名称 image: nacos/nacos-server:latest # 使用的镜像 container_name: nacos-standalone-mysql # 容器名称 env_file: # 环境变量文件加载 - ../env/nacos-standlone-mysql.env volumes: # 宿主与容器文件挂载 - ./standalone-logs/:/home/nacos/logs - ./init.d/custom.properties:/home/nacos/init.d/custom.properties ports: # 宿主与容器间端口映射 - "8848:8848" - "9555:9555" depends_on: # 依赖的服务 - mysql restart: on-failure # 重启策略 mysql: # 数据库基础服务 container_name: mysql image: nacos/nacos-mysql:5.7 env_file: - ../env/mysql.env volumes: - ./mysql:/var/lib/mysql ports: - "3306:3306" prometheus: # prometheus 用于监控nacos服务，为grafana提供基础数据 container_name: prometheus image: prom/prometheus:latest volumes: - ./prometheus/prometheus-standalone.yaml:/etc/prometheus/prometheus.yml ports: - "9090:9090" depends_on: - nacos restart: on-failure grafana: # 监控面板，配合prometheus使用 container_name: grafana image: grafana/grafana:latest ports: - 3000:3000 restart: on-failure Key Concepts 架构 从架构图可以看出 Docker Client使用Swarm对 集群(Cluster)进行调度使用。 上图可以看出，Swarm是典型的master-slave结构，通过发现服务来选举manager。manager是中心管理节点，各个node上运行agent接受manager的统一管理，集群会自动通过Raft协议分布式选举出manager节点，无需额外的发现服务支持，避免了单点的瓶颈问题，同时也内置了DNS的负载均衡和对外部负载均衡机制的集成支持。 node（节点）: 一个节点是docker引擎集群的一个实例，也可以将其视为一个Docker节点。可以在单个物理计算机或云服务器上运行一个或多个节点，但在部署生产集群时通常将节点分布部署在多个物理和云计算机上。要将应用程序部署到swarm，需要将服务定义信息提交给 manager node(管理节点)。管理节点将称为tasks(任务)的工作单元分派给 worker nodes(工作节点)。管理节点还执行编排和集群管理功能，从而使群集处于需要的状态。管理节点会选举一个领导者来执行编排任务。工作节点接收并执行从管理器节点分派的任务。默认情况下，管理器节点也可以作为工作节点运行，但可以将它们配置为仅运行管理任务并且仅是管理节点。代理程序在每个工作节点上运行着一个代理程序，这个代理程序会报告分配给这个工作节点的任务的详细信息。工作节点会通知管理节点分配给它的任务的当前状态，以便管理节点可以维护每个工作节点的状态。 service(服务)：服务是要执行在节点上的一组task（任务）的定义，它是集群系统的核心结构，同时也是用户与swarm系统进行交互的核心内容。Service有两种运行模式，一种是replicated，swarm会将服务所需的任务按照你定义的数量复制部署在多个节点上；另一种是global，在所有符合运行条件的Node上，都运行一个服务所需的任务。 task(任务):一个任务是一个docker容器和容器内运行命令的集合。它是swarm任务调度的原子单位，管理节点会根据服务内对任务的定义将任务分派到工作节点，一旦一个任务分配到了一个节点，这个任务就不能再移动都其它节点，他只能再一开始分派到的节点上运行或者直到失败。 负载均衡：swarm使用了入口负载均衡器来把服务开放给swarm外部，swarm可以自动给服务分配外部端口也可以由用户自定义外部端口，如果用户不定义外部端口的话，swarm会在30000~3276这个范围内随机给服务分配一个端口。外部组件比如云负载均衡器或者nginx之类的组件，可以从集群内的任意一个节点（无论该节点是否在运行要访问服务的任务）来访问服务。所有集群内节点都可以路由入口连接到一个运行着任务的实例。另外swarm模式还有一个内部DNS组件来给服务自动分派一个DNS入口。swarm manager使用一个内部负载均衡器根据每个服务的DNS地址来分配集群内服务的请求。 Why为什么使用docker swarm，其实也就是一些功能特点，满足了需求就用： 与Docker Engine集成的集群管理:使用Docker Engine CLI创建一组Docker引擎，您可以在其中部署应用程序服务。您不需要其他编排软件来创建或管理群集。 节点分散式设计：Docker Engine不是在部署时处理节点角色之间的差异，而是在运行时处理角色变化。您可以使用Docker Engine部署两种类型的节点，管理节点和工作节点。这意味着您可以很方便地从单个镜像开始构建整个群集。 声明式服务模型：Docker Engine使用声明式方法来定义应用程序堆栈中各种服务的所需状态。例如，您可以描述由具有消息队列服务和数据库后端的Web前端服务组成的应用程序。 可扩容与缩放容器：对于每个服务，您可以声明要运行的任务数。当您扩容或缩放时，swarm管理器通过添加或删除任务来自动适应，以保持所需的任务数量来保证集群的可靠状态。 容器容错状态协调：集群管理节点不断监视群集状态，并协调期望状态与实际状态之间的任何差异。例如，如果设置了一个运行10个副本容器的服务，然后托管其中两个副本容器的工作节点崩溃了，那么管理器将创建两个新副本容器以替换崩溃的副本容器。 swarm管理器将新副本容器分配给正在运行和可用的工作节点上。 多主机网络：您可以为服务指定承载网络。当swarm管理器初始化或更新应用程序时，它会自动为承载网络上的容器分配地址。 服务发现：Swarm管理节点为swarm中的每个服务分配唯一的DNS名称，并对运行的容器进行负载均衡。您可以通过嵌入在swarm中的DNS服务器查询在集群中运行的每个容器。 负载均衡：您可以将服务的端口公开给外部负载平衡器。在内部，swarm允许您指定如何在节点之间分发服务容器。 缺省安全：集群中的每个节点强制执行TLS验证和加密，以保护其自身与所有其他节点之间的通信。可以选择使用自签名根证书或来自自定义根CA的证书。 滚动更新：在已经运行期间，您可以增量地应用服务更新到节点。 swarm管理器允许您控制将服务部署到不同节点集之间的延迟。如果出现任何问题，您可以将任务回滚到服务的先前版本。 How Set up(一些准备)： 三台linux机器，之间网络互通并且都安装了docker(最好同一版本，安装方法可以参照以前的文章) 三台机器中选一台作为manager节点，记住该机器的ip,确保其它节点可以通过该ip访问到manager节点 开放三台机器的对应端口：2377(TCP，用于集群管理通讯),7946(TCP and UDP，用于节点间通讯),4789(UDP,用于服务负载网络) Create a swarm(创建集群)： ssh连接到选定的管理节点上 在管理节点上运行如下命令 1$ docker swarm init --advertise-addr &lt;MANAGER-IP&gt; 注意运行命令后的输出，其中docker swarm join命令就是用来将其它工作节点加入集群的命令,最后的docker swarm join-token manager是用于向集群内新增manager节点的，因此要记住token： 12345678910$ docker swarm init --advertise-addr 192.168.99.100Swarm initialized: current node (dxn1zf6l61qsb1josjja83ngz) is now a manager.To add a worker to this swarm, run the following command: docker swarm join \ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \ 192.168.99.100:2377To add a manager to this swarm, run &apos;docker swarm join-token manager&apos; and follow the instructions. 如果不小心忘记了token，可以在管理节点上运行docker swarm join-token worker来查看自动生成的加入工作节点命令 在管理节点上运行docker info命令可以看到如下输出显示当前集群的状态： 12345678910111213$ docker infoContainers: 2Running: 0Paused: 0Stopped: 2 ...snip...Swarm: active NodeID: dxn1zf6l61qsb1josjja83ngz Is Manager: true Managers: 1 Nodes: 1 ...snip... 在管理节点上运行docker node ls命令可以看到如下输出显示当前集群节点信息，其中带*的这行表示你目前连接到了这个节点上： 1234$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUSdxn1zf6l61qsb1josjja83ngz * manager1 Ready Active Leader 向集群中加入节点： ssh连接到剩余的两台作为工作节点的机器上 运行之前记录下来的加入工作节点的命令，具体如下： 12345$ docker swarm join \ --token SWMTKN-1-49nj1cmql0jkz5s954yi3oex3nedyz0fb0xx14ie39trti4wxv-8vxv8rssmk743ojnwacrr2e7c \ 192.168.99.100:2377This node joined a swarm as a worker. ssh回到管理节点，运行docker node ls命令来查看两个工作节点是否被成功加入集群，具体如下： 12345$ docker node lsID HOSTNAME STATUS AVAILABILITY MANAGER STATUS03g1y59jwfg7cf99w4lt0f662 worker2 Ready Active9j68exjopxe7wfl6yuxml7a7j worker1 Ready Activedxn1zf6l61qsb1josjja83ngz * manager1 Ready Active Leader 部署服务到集群： ssh连接到管理节点上 运行如下命令： 1$ docker service create --replicas 1 --name helloworld alpine ping docker.com docker service create: 用于创建服务的命令 –name helloworld：服务名称叫做helloworld –replicas 1: 服务的任务副本（或者说容器数量）只保留一个副本 alpine ：镜像名 ，代表 Alpine Linux container ping docker.com: 容器内运行的命令 运行docker service ls查看运行中的服务，具体如下图： 1234$ docker service lsID NAME SCALE IMAGE COMMAND9uk4639qpg7n helloworld 1/1 alpine ping docker.com 查看服务： 查看服务详细信息： 1234567891011121314[manager1]$ docker service inspect --pretty helloworldID: 9uk4639qpg7npwf3fn2aasksrName: helloworldService Mode: REPLICATED Replicas: 1Placement:UpdateConfig: Parallelism: 1ContainerSpec: Image: alpine Args: ping docker.comResources:Endpoint Mode: vip 查看服务运行节点： 1234[manager1]$ docker service ps helloworldNAME IMAGE NODE DESIRED STATE CURRENT STATE ERROR PORTShelloworld.1.8p1vev3fq5zm0mi8g0as41w35 alpine worker2 Running Running 3 minutes 在服务运行节点上使用docker ps 查看运行中的容器： 1234[worker2]$ docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMESe609dde94e47 alpine:latest "ping docker.com" 3 minutes ago Up 3 minutes helloworld.1.8p1vev3fq5zm0mi8g0as41w35 扩容服务： 使用 docker service scale = 来扩容，具体如下： 123$ docker service scale helloworld=5helloworld scaled to 5 查看扩容后的结果： 12345678$ docker service ps helloworldNAME IMAGE NODE DESIRED STATE CURRENT STATEhelloworld.1.8p1vev3fq5zm0mi8g0as41w35 alpine worker2 Running Running 7 minuteshelloworld.2.c7a7tcdq5s0uk3qr88mf8xco6 alpine worker1 Running Running 24 secondshelloworld.3.6crl09vdcalvtfehfh69ogfb1 alpine worker1 Running Running 24 secondshelloworld.4.auky6trawmdlcne8ad8phb0f1 alpine manager1 Running Running 24 secondshelloworld.5.ba19kca06l18zujfwxyc5lkyn alpine worker2 Running Running 24 seconds 删除服务: 使用docker service rm 即可删除服务，具体如下 123$ docker service rm helloworldhelloworld 滚动更新服务： 以redis 为例，先创建一个有三个副本的redis服务： 1234567$ docker service create \ --replicas 3 \ --name redis \ --update-delay 10s \ redis:3.0.60u6a4s31ybk7yw2wyvtikmu50 注意通过–update-delay声明了服务更新的延迟时间为10秒，也可以通过 单独或组合使用10s,10m,10h来表示不同的延迟，比如10m30s代表十分三十秒 开始更新，将redis版本进行升级： 12$ docker service update --image redis:3.0.7 redisredis 升级过程中可以使用docker service inspect 来查看升级状况,如下图是升级失败： 12345678910docker service inspect --pretty redisID: 0u6a4s31ybk7yw2wyvtikmu50Name: redis...snip...Update status: State: paused Started: 11 seconds ago Message: update paused due to failure or early termination of task 9p7ith557h8ndf0ui9s0q951b...snip... 升级失败了重新运行升级命令docker service update redis即可 升级过程中可以使用docker service ps来查看升级状况，有的会是3.0.6 running,有的是3.0.7running,一旦升级完毕则全部都是3.0.7 running如下图 ： 123456789$ docker service ps redisNAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORredis.1.dos1zffgeofhagnve8w864fco redis:3.0.7 worker1 Running Running 37 seconds \_ redis.1.88rdo6pa52ki8oqx6dogf04fh redis:3.0.6 worker2 Shutdown Shutdown 56 seconds agoredis.2.9l3i4j85517skba5o7tn5m8g0 redis:3.0.7 worker2 Running Running About a minute \_ redis.2.66k185wilg8ele7ntu8f6nj6i redis:3.0.6 worker1 Shutdown Shutdown 2 minutes agoredis.3.egiuiqpzrdbxks3wxgn8qib1g redis:3.0.7 worker1 Running Running 48 seconds \_ redis.3.ctzktfddb2tepkr45qcmqln04 redis:3.0.6 mmanager1 Shutdown Shutdown 2 minutes ago 排空一个节点： 某些情况下要把某个节点下线或者关机时，需要先将节点上的tasks转移，这个过程就叫做排空,如下图 123docker node update --availability drain worker1worker1 再运行docker service ps redis，可以看到worker1上的容器都停止了并在worker2上开了新的容器： 1234567$ docker service ps redisNAME IMAGE NODE DESIRED STATE CURRENT STATE ERRORredis.1.7q92v0nr1hcgts2amcjyqg3pq redis:3.0.6 manager1 Running Running 4 minutesredis.2.b4hovzed7id8irg1to42egue8 redis:3.0.6 worker2 Running Running About a minute \_ redis.2.7h2l8h3q3wqy5f66hlv9ddmi6 redis:3.0.6 worker1 Shutdown Shutdown 2 minutes agoredis.3.9bg7cezvedmkgg6c8yzvbhwsd redis:3.0.6 worker2 Running Running 4 minutes 当你需要使排空的节点可用，运行以下命令即可，之后该节点就可以正常接受新的任务了： 123$ docker node update --availability active worker1worker1 暴露服务端口 如docker中使用-p 8080:80一样，也可以使用同样的方式暴露服务端口，但下面的命令更为清晰： 12345$ docker service create \ --name my-web \ --publish published=8080,target=80 \ --replicas 2 \ nginx 之后你访问三台机器的 8080端口，都可以访问到nginx，即使那台机器上没有运行nginx的任务——这个特性被叫做routing mesh(路由混合) 需要更新服务端口的话可以使用如下命令 123$ docker service update \ --publish-add published=&lt;PUBLISHED-PORT&gt;,target=&lt;CONTAINER-PORT&gt; \ &lt;SERVICE&gt; 默认暴露的是TCP端口，如果要暴露UDP端口需要如下设置 123456789$ docker service create --name dns-cache \ --publish published=53,target=53 \ --publish published=53,target=53,protocol=udp \ dns-cache 或者$ docker service create --name dns-cache \ -p 53:53 \ -p 53:53/udp \ dns-cache 跳过路由混合： 如果不想使用以上“访问三台机器的 8080端口，都可以访问到nginx，即使那台机器上没有运行nginx的任务”这个特性，可以通过对端口声明mode的方式进行控制，如下所示： 1234$ docker service create --name dns-cache \ --publish published=53,target=53,protocol=udp,mode=host \ --mode global \ dns-cache 这种情况下，访问机器的53端口，就要求那台机器上必须要有运行中的task，并且访问的就是那个task 使用服务承载网络： 如果集群中多个服务需要互相通讯，就可以使用承载网络，如下所示: 先创建网络 1$ docker network create --driver overlay my-network 创建服务时声明其所在网络 12345$ docker service create \ --replicas 3 \ --network my-network \ --name my-web \ nginx 将已存在的服务加入到某个网络 1docker service update --network-add my-network my-web Docker stackDocker Compose缺点是不能在分布式多机器上使用；Docker swarm缺点是不能同时编排多个服务，所以才有了Docker Stack，可以在分布式多机器上同时编排多个服务。stack就是一组共用一个承载网络的服务的集合。 例子： Setup service1: 123456789@Slf4j@RestControllerpublic class HelloRest &#123; @GetMapping("/service1/getHello") public String getHello()&#123; log.info("service1!!!"); return "hello from service1"; &#125;&#125; 1234FROM openjdk:8EXPOSE 8080ADD target/service1-0.0.1-SNAPSHOT.jar /demo.jarENTRYPOINT ["java", "-jar", "demo.jar"] service2: 123456789@Slf4j@RestControllerpublic class HelloRest &#123; @GetMapping("/service2/getHello") public String getHello()&#123; log.info("service2!!!"); return "hello from service2"; &#125;&#125; 1234FROM openjdk:8EXPOSE 8081ADD target/service2-0.0.1-SNAPSHOT.jar /demo.jarENTRYPOINT ["java", "-jar", "demo.jar"] 分别打包镜像 12docker build -t service1:V1 .docker build -t service2:V1 . 编写docker-compose.yml文件 123456789101112131415version: "3.9"services: service1: image: "zhangjie47/service1:V1" deploy: replicas: 2 ports: - "8080:8080" service2: image: "zhangjie47/service2:V1" deploy: replicas: 3 ports: - "8081:8081" stack 部署 1234567# myapps是stack的自定义名称，使用具体路径的compose配置文件进行部署docker stack deploy myapps --compose-file=docker-compose.yml或者docker stack deploy myapps -c docker-compose.ymlCreating network myapps_defaultCreating service myapps_service1Creating service myapps_service2 查看docker stack 123456789101112131415161718192021222324252627# 查看所有stack的信息docker stack lsNAME SERVICES ORCHESTRATORmyapps 2 Swarm# 查看某个stack中的所有任务信息docker stack ps myappsID NAME IMAGE NODE DESIRED STATE CURRENT STATEtvvujrf3qcr1 myapps_service1.1 zhangjie47/service1:V1 node1 Running Running 46 seconds ago igjeydmmvzzm myapps_service1.2 zhangjie47/service1:V1 manager Running Running 46 seconds ago 7p5c96eplwl3 myapps_service2.1 zhangjie47/service2:V1 node1 Running Running 34 seconds ago 7shglsajip5d myapps_service2.2 zhangjie47/service2:V1 manager Running Running 39 seconds ago upo0mr7j9tn1 myapps_service2.3 zhangjie47/service2:V1 node2 Running Preparing 41 seconds ago # 查看某个stack中的所有服务信息docker stack services myappsID NAME MODE REPLICAS IMAGE PORTSicz3kjn0skb3 myapps_service1 replicated 2/2 masonzhang/service1:V1 *:8080-&gt;8080/tcpmyuzlwnrxag4 myapps_service2 replicated 3/3 masonzhang/service2:V1 *:8081-&gt;8081/tcp# 查看具体服务信息等请参照上文# 移除stackdocker stack rm myappsRemoving service myapps_service1Removing service myapps_service2Removing network myapps_default]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>云原生</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python项目容器化打包规范]]></title>
    <url>%2Fblog%2F2022%2F11%2F23%2FPython%E9%A1%B9%E7%9B%AE%E5%AE%B9%E5%99%A8%E5%8C%96%E6%89%93%E5%8C%85%E8%A7%84%E8%8C%83%2F</url>
    <content type="text"><![CDATA[引言一个成熟的python项目可能会依赖很多特定的环境，然而项目运行的结果不仅取决于代码，和运行代码的环境也息息相关。这很有可能会造成，开发环境上的运行结果和测试环境、线上环境上的结果都不一致的现象。为了解决这个问题，我们可以将python项目打包成docker镜像，这样即使在不同的机器上运行打包后的项目，我们也能够得到一致的运行结果。 准备工作安装docker可以使用如下脚本一键安装 1sudo curl -sSL https://get.daocloud.io/docker | sh 也可参考 CentOS Docker基础环境安装 准备python项目在项目根目录下添加以下两个文件 requirements.txt 该文件描述了python项目的依赖环境，使用如下命令即可导出相关库的信息并自动生成依赖文件 1pip freeze &gt; requirements.txt Dockerfile 123456789101112131415161718# 将官方 Python 运行时用作父镜像，根据需要选择python版本FROM python:3.7# 记录维护这信息MAINTAINER zhangjie@parkson.com.cn# 将工作目录设置为 /myappsWORKDIR ./myapps# 将当前目录内容复制到位于 /myapps 中的容器中ADD . .# 安装 requirements.txt 中指定的任何所需软件包RUN pip install -r requirements.txt# 使用匿名卷持久化存储日志或者其它数据，匿名卷实际地址在docker安装目录下，可以用docker inspect查看VOLUME [/logs1,/logs2]# 定义环境变量ENV name1=var1 name2=var2# 暴露端口配置，默认TCP,可指定为udpEXPOSE [80/tcp,80/udp]# 在容器启动时运行 hello_world.pyENTRYPOINT ["python", "./myapps/hello_world.py"] note:有时候pip install -r requirements.txt很慢，可以考虑换源，只需将原命令改为如下即可 1RUN pip install -r requirements.txt -i https://pypi.douban.com/simple 打包镜像在Dockerfile所在的目录下运行 1sudo docker build -t demo:v1 . 其中demo为镜像名称，v1为镜像tag用于版本管理 运行容器1sudo docker run demo:v1 其它常用参数，根据需要使用： -d: 后台运行容器，并返回容器ID； -v /opt/logs:/logs 挂载目录,将容器中的目录映射到宿主机中，格式为宿主机目录:容器中的目录 -p 80：80 指定端口映射，格式为：主机(宿主)端口:容器端口 -e username=”ritchie” 设置环境变量； –env-file .env 使用.env文件设置环境变量 –name nginx1.18 为容器指定一个名称； –link mysql:mysql 添加链接到另一个容器； 标准化自动打包、自动部署流程本规范仅专注于如何打包python容器，如果需要结合jenkins自动化部署流程，则需要参照以下步骤 添加docker.sh文件需要在项目根目录添加docker.sh文件，参照如下: 设置镜像名称BUILD_IMAGE_NAME不需要设置tag，统一使用构建时间生成，构建成功后保存在.docker_build_version文件中根据项目设置run()函数容器的映射端口 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314315316317318319320321322323324325326327328329330331332333334335336337338339340341342343344345346347348349350351352353354355#!/usr/bin/env bash# 获取脚本所在文件件BUILD_DIRECTORY=$(dirname $(readlink -f "$0"))# docker仓库相关信息BUILD_REGISTRY=$DOCKER_REGISTRYBUILD_REGISTRY_USER=$DOCKER_REGISTRY_USERBUILD_REGISTRY_PASS=$DOCKER_REGISTRY_PASS# docker生产仓库相关信息BUILD_REGISTRY_PROD=$DOCKER_REGISTRY_PRODBUILD_REGISTRY_PROD_USER=$DOCKER_REGISTRY_PROD_USERBUILD_REGISTRY_PROD_PASS=$DOCKER_REGISTRY_PROD_PASS# 命名空间BUILD_NAMESPACE=""# 镜像名称BUILD_IMAGE_NAME=aglaia-web-reif [ "$2" ]; then BUILD_IMAGE_NAME=$2fi# 镜像版本BUILD_VERSION=""# 最终镜像名BUILD_IMAGE=$BUILD_IMAGE_NAMEif [ "$BUILD_NAMESPACE" ]; then BUILD_IMAGE=$BUILD_NAMESPACE/$BUILD_IMAGEfi# 检查docker仓库配置check_docker_registry() &#123; if [ "$BUILD_REGISTRY" == "" ]; then echo "请配置docker仓库环境变量：DOCKER_REGISTRY" exit 1 fi&#125;# 检查docker仓库认证信息check_docker_registry_user_and_pass() &#123; if [ "$BUILD_REGISTRY_USER" == "" -o "$BUILD_REGISTRY_PASS" == "" ]; then echo "请配置docker仓库认证信息环境变量：DOCKER_REGISTRY_USER、DOCKER_REGISTRY_PASS" exit 1 fi&#125;# 检查docker生产仓库配置check_docker_registry_prod() &#123; if [ "$BUILD_REGISTRY_PROD" == "" ]; then echo "请配置生产docker仓库环境变量：DOCKER_REGISTRY_PROD" exit 1 fi&#125;# 检查docker生产仓库认证信息check_docker_registry_prod_user_and_pass() &#123; if [ "$BUILD_REGISTRY_PROD_USER" == "" -o "$BUILD_REGISTRY_PROD_PASS" == "" ]; then echo "请配置生产docker仓库认证信息环境变量：DOCKER_REGISTRY_PROD_USER、DOCKER_REGISTRY_PROD_PASS" exit 1 fi&#125;# 检查镜像check_image() &#123; if [ "$BUILD_IMAGE" == "" ]; then echo "请配置镜像名称：BUILD_IMAGE_NAME，或通过参数指定 docker.sh &lt;command&gt; &lt;image&gt;" exit 1 fi&#125;# 检查版本check_version() &#123; if [ "$BUILD_VERSION" == "" ]; then echo "请先执行build操作" exit 1 fi&#125;# 生产版本号gen_version() &#123; echo $(date "+%Y%m%d%H%M%S")&#125;# 获取版本号get_version() &#123; local FILE=$BUILD_DIRECTORY/.docker_build_version if [ -f "$FILE" ]; then echo $(cat "$FILE") else echo "" fi&#125;# 当需要时执行构建build_when_necessary() &#123; if [ "$(get_version)" == "" ]; then build fi&#125;# 构建镜像build() &#123; echo "构建镜像" check_docker_registry check_image BUILD_VERSION=$(gen_version) check_version docker build --no-cache --force-rm $BUILD_DIRECTORY -t $BUILD_REGISTRY/$BUILD_IMAGE:$BUILD_VERSION if [ "$?" != "0" ]; then echo "构建镜像失败" exit 1 fi echo $BUILD_IMAGE &gt;$BUILD_DIRECTORY/.docker_build_image echo $BUILD_VERSION &gt;$BUILD_DIRECTORY/.docker_build_version echo "构建镜像成功"&#125;# 执行镜像run() &#123; echo "执行镜像" check_docker_registry check_image BUILD_VERSION=$(get_version) check_version docker run --rm -p 80:80 $BUILD_REGISTRY/$BUILD_IMAGE:$BUILD_VERSION if [ "$?" != "0" ]; then echo "执行镜像失败" exit 1 fi echo "执行镜像成功"&#125;# 推送镜像push() &#123; echo "推送镜像" check_docker_registry check_docker_registry_user_and_pass check_image BUILD_VERSION=$(get_version) check_version docker login $BUILD_REGISTRY -u $BUILD_REGISTRY_USER -p $BUILD_REGISTRY_PASS if [ "$?" != "0" ]; then echo "登录docker仓库失败" exit 1 fi docker push $BUILD_REGISTRY/$BUILD_IMAGE:$BUILD_VERSION if [ "$?" != "0" ]; then echo "推送镜像失败" exit 1 fi echo "推送镜像成功"&#125;# 标记为最新版本tag_latest() &#123; echo "标记为最新版本" check_docker_registry check_image BUILD_VERSION=$(get_version) check_version docker tag $BUILD_REGISTRY/$BUILD_IMAGE:$BUILD_VERSION $BUILD_REGISTRY/$BUILD_IMAGE:latest if [ "$?" != "0" ]; then echo "标记为最新版本失败" exit 1 fi echo "标记为最新版本成功"&#125;# 推送最新版本push_latest() &#123; echo "推送最新版本" check_docker_registry check_docker_registry_user_and_pass check_image BUILD_VERSION=$(get_version) check_version docker login $BUILD_REGISTRY -u $BUILD_REGISTRY_USER -p $BUILD_REGISTRY_PASS if [ "$?" != "0" ]; then echo "登录docker仓库失败" exit 1 fi docker push $BUILD_REGISTRY/$BUILD_IMAGE:latest if [ "$?" != "0" ]; then echo "推送最新版本失败" exit 1 fi echo "推送最新版本成功"&#125;# 标记为生产版本tag_prod() &#123; echo "标记为生产版本" check_docker_registry check_docker_registry_prod check_image BUILD_VERSION=$(get_version) check_version docker tag $BUILD_REGISTRY/$BUILD_IMAGE:$BUILD_VERSION $BUILD_REGISTRY_PROD/$BUILD_IMAGE:$BUILD_VERSION if [ "$?" != "0" ]; then echo "标记为生产版本失败" exit 1 fi echo "标记为生产版本成功"&#125;# 推送生产版本push_prod() &#123; echo "推送生产版本" check_docker_registry check_docker_registry_prod check_docker_registry_prod_user_and_pass check_image BUILD_VERSION=$(get_version) check_version docker login $BUILD_REGISTRY_PROD -u $BUILD_REGISTRY_PROD_USER -p $BUILD_REGISTRY_PROD_PASS if [ "$?" != "0" ]; then echo "登录docker生产仓库失败" exit 1 fi docker push $BUILD_REGISTRY_PROD/$BUILD_IMAGE:$BUILD_VERSION if [ "$?" != "0" ]; then echo "推送生产版本失败" exit 1 fi echo "推送生产版本成功"&#125;# 标记为生产最新版本tag_prod_latest() &#123; echo "标记为生产最新版本" check_docker_registry check_docker_registry_prod check_image BUILD_VERSION=$(get_version) check_version docker tag $BUILD_REGISTRY/$BUILD_IMAGE:$BUILD_VERSION $BUILD_REGISTRY_PROD/$BUILD_IMAGE:latest if [ "$?" != "0" ]; then echo "标记为生产最新版本失败" exit 1 fi echo "标记为生产最新版本成功"&#125;# 推送生产最新版本push_prod_latest() &#123; echo "推送生产最新版本" check_docker_registry check_docker_registry_prod check_docker_registry_prod_user_and_pass check_image BUILD_VERSION=$(get_version) check_version docker login $BUILD_REGISTRY_PROD -u $BUILD_REGISTRY_PROD_USER -p $BUILD_REGISTRY_PROD_PASS if [ "$?" != "0" ]; then echo "登录docker生产仓库失败" exit 1 fi docker push $BUILD_REGISTRY_PROD/$BUILD_IMAGE:latest if [ "$?" != "0" ]; then echo "推送生产最新版本失败" exit 1 fi echo "推送生产最新版本成功"&#125;RC=0case "$1" inbuild) build ;;run) run ;;push) push ;;tag-latest) tag_latest ;;push-latest) push_latest ;;tag-prod) tag_prod ;;push-prod) push_prod ;;tag-prod-latest) tag_prod_latest ;;push-prod-latest) push_prod_latest ;;pipeline) build_when_necessary push tag_latest push_latest ;;pipeline-prod) build_when_necessary tag_prod push_prod tag_prod_latest push_prod_latest ;;pipeline-all) build_when_necessary push tag_latest push_latest tag_prod push_prod tag_prod_latest push_prod_latest ;;pipeline-rebuild) build push tag_latest push_latest ;;pipeline-prod-rebuild) build tag_prod push_prod tag_prod_latest push_prod_latest ;;pipeline-all-rebuild) build push tag_latest push_latest tag_prod push_prod tag_prod_latest push_prod_latest ;;*) echo $"Usage 1: $0 &#123;build|run|push|tag-latest|push-latest|tag-prod|push-prod|tag-prod-latest|push-prod-latest&#125;" echo $"Usage 2: $0 &#123;pipeline|pipeline-prod|pipeline-all|pipeline-rebuild|pipeline-prod-rebuild|pipeline-all-rebuild&#125;" echo $"Tips 1: 使用前请设置脚本变量：BUILD_NAMESPACE、BUILD_IMAGE_NAME" echo $"Tips 2: 使用前请设置环境变量：DOCKER_REGISTRY、DOCKER_REGISTRY_USER、DOCKER_REGISTRY_PASS、DOCKER_REGISTRY_PROD、DOCKER_REGISTRY_PROD_USER、DOCKER_REGISTRY_PROD_PASS" exit 2 ;;esacexit $RC 赋予docker.sh执行权限1sudo chmod a+x docker.sh 设置dockerhub相关环境变量 这些环境变量在docker.sh中会用到，并且所有docker打包发布相关操作都会用到，因此强烈建议直接配置到/etc/profile 123456789# docker仓库相关信息export DOCKER_REGISTRY=docker.parkson.net.cnexport DOCKER_REGISTRY_USER=&lt;账号&gt;export DOCKER_REGISTRY_PASS=&lt;密码&gt;# docker生产仓库相关信息export DOCKER_REGISTRY_PROD=docker-registry.parkson.net.cnexport DOCKER_REGISTRY_PROD_USER=&lt;账号&gt;export DOCKER_REGISTRY_PROD_PASS=&lt;密码&gt; docker.sh使用方法查看使用方法1./docker.sh 构建镜像1./docker.sh build 运行镜像1./docker.sh run 推送镜像1./docker.sh push 标记为最新版本1./docker.sh tag-latest 推送最新版本1./docker.sh push-latest 标记为生产版本1./docker.sh tag-prod 推送生产版本1./docker.sh push-prod 标记为生产最新版本1./docker.sh tag-prod-latest 推送生产最新版本1./docker.sh push-prod-latest 执行流水线作业 build(如果已经构建不会重新构建)-&gt;push-&gt;tag-latest-&gt;push-latest 1./docker.sh pipeline 执行生产流水线作业 build(如果已经构建不会重新构建)-&gt;tag-prod&gt;push-prod-&gt;tag-latest-&gt;push-latest 1./docker.sh pipeline-prod 执行全部流水线作业 build(如果已经构建不会重新构建)-&gt;push-&gt;tag-latest-&gt;push-latest-&gt;tag-prod&gt;push-prod-&gt;tag-latest-&gt;push-latest 1./docker.sh pipeline-all 执行流水线作业（强制重新构建镜像） build(每次都重新构建镜像)-&gt;push-&gt;tag-latest-&gt;push-latest 1./docker.sh pipeline-rebuild 执行生产流水线作业（强制重新构建镜像） build(每次都重新构建镜像)-&gt;tag-prod&gt;push-prod-&gt;tag-latest-&gt;push-latest 1./docker.sh pipeline-prod-rebuild 执行全部流水线作业（强制重新构建镜像） build(每次都重新构建镜像)-&gt;push-&gt;tag-latest-&gt;push-latest-&gt;tag-prod&gt;push-prod-&gt;tag-latest-&gt;push-latest 1./docker.sh pipeline-all-rebuild 项目相关环境变量配置在项目根目录下添加.env文件（用于设置环境变量，更多环境变量请根据项目进行设置） .env 1234# dev、test、prodENV=dev# DBURIHOST=10.88.1.12 docker-compose.yml中环境变量的优先级需要区分两种环境变量，一种指传递到容器内部的环境变量，一种指docker-compose.yml文件中通过${}符号引用用于设置docker-compose的环境变量。 前者的优先级从低到高如下： docker-compose.yml中声明env_file docker-compose.yml声明environment 后者的优先级从低到高如下： 同一目录下放一个.env文件 在/etc/profile或者在shell中通过export设置环境变量 注意，存在.env文件在前者和后者都使用的情况，此种情况各论各即可 添加docker-compose.yml文件在项目根目录下添加docker-compose.yml文件 docker-compose.yml设置镜像名称（格式：项目代号，形如xxx-yyy-zzz，例如aglaia-ui-re）容器名称与镜像名称保持一致主机名称与镜像名称保持一致根据项目设置容器的映射端口大部分基底容器默认不支持中文，为了保持统一只设置时区为Asia/Shanghai，语言方面以英语作为默认语言 12345678910111213141516171819202122232425version: "3.5"services: &lt;项目代号&gt;: image: $&#123;DOCKER_REGISTRY:-docker.parkson.net.cn&#125;/&lt;项目代号&gt;:$&#123;VERSION:-latest&#125; container_name: &lt;项目代号&gt; hostname: &lt;项目代号&gt; ports: - 8080:8080 ulimits: memlock: soft: -1 hard: -1 env_file: - .env environment: - TZ=Asia/Shanghai - LANG=en_US.UTF-8 - LANGUAGE=en_US:en - LC_ALL=en_US.UTF-8 restart: unless-stopped logging: driver: "json-file" options: max-size: "128m" max-file: "20" docker-compose.yaml使用方法该文件主要用于部署配合Jenkins自动化部署使用 将docker-compose.yml上传到服务器的”/docker/compose/&lt;项目代号&gt;/“目录下 启动容器 前台 1docker-compose up 启动容器 后台 1docker-compose up -d 销毁容器1docker-compose down 更新镜像1docker-compose pull 查看日志 查看最后500行日志 1docker-compose logs --tail 500 查看最后500行日志（滚动刷新） 1docker-compose logs --tail 500 -f]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[CentOS Docker基础环境安装]]></title>
    <url>%2Fblog%2F2022%2F11%2F23%2FCentOS-Docker%E5%9F%BA%E7%A1%80%E7%8E%AF%E5%A2%83%E5%AE%89%E8%A3%85%2F</url>
    <content type="text"><![CDATA[环境准备切换到root账户1su - root 安装wget1yum install wget -y 备份原始Yum源1mv /etc/yum.repos.d/CentOS-Base.repo /etc/yum.repos.d/CentOS-Base.repo.backup 下载新的Yum源1wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.aliyun.com/repo/Centos-7.repo 安装EPEL源1yum install epel-release -y 备份原始EPEL源12mv /etc/yum.repos.d/epel.repo /etc/yum.repos.d/epel.repo.backupmv /etc/yum.repos.d/epel-testing.repo /etc/yum.repos.d/epel-testing.repo.backup 下载新的EPEL源1wget -O /etc/yum.repos.d/epel.repo https://mirrors.aliyun.com/repo/epel-7.repo 清除Yum缓存1yum clean all 构建Yum缓存1yum makecache 安装常用工具组1234yum groupinstall -y &quot;Compatibility Libraries&quot; \ &quot;Console Internet Tools&quot; \ &quot;Security Tools&quot; \ &quot;Development Tools&quot; 安装常用工具包123456789101112131415161718192021222324252627282930313233yum install -y yum-utils \ nfs-utils \ bind-utils \ bridge-utils \ httpd-tools\ net-tools \ device-mapper-persistent-data \ lvm2 \ ipvsadm \ htop \ tzdata \ nscd \ openssl \ vim \ neovim \ curl \ wget \ zip \ unzip \ bzip2 \ git \ subversion \ perl \ ruby \ python \ python3 \ lrzsz \ tree \ finger \ lsof \ telnet \ nmap \ nc 安装X11图形支持（按需）123yum install -y xorg-x11-xauth \ gedit \ gvim 禁用yum自动更新（根据实际需要选择）停止yum自动更新服务1systemctl stop yum-cron 禁用yum自动更新服务1systemctl disable yum-cron 查看yum自动更新服务状态1systemctl status yum-cron 设置系统时区12345ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeecho &quot;Asia/Shanghai&quot; &gt; /etc/timezonecat /etc/timezoneecho &quot;export TZ=&apos;Asia/Shanghai&apos;&quot; &gt; /etc/profile.d/tz.shcat /etc/profile.d/tz.sh 设置语言（按需）12echo &quot;LANG=\&quot;zh_CN.UTF-8\&quot;&quot; &gt; /etc/locale.confcat /etc/locale.conf NTP服务安装NTP服务1yum install ntpdate -y 设置时间同步上游服务器 编辑 1vim /etc/ntp/step-tickers 修改 123456780.centos.pool.ntp.org=&gt;# 国家授时中心ntp.ntsc.ac.cn# 亚洲授时中心asia.pool.ntp.org# 阿里云授时中心ntp.aliyun.com 启用ntpdate服务1systemctl enable ntpdate 启动ntpdate服务1systemctl restart ntpdate 有时启动不成功，多试几次，还不成功就刷新dns缓存 1systemctl restart nscd 查看ntpdate服务状态1systemctl status ntpdate 防火墙停止防火墙1systemctl stop firewalld 禁用防火墙1systemctl disable firewalld 查看防火墙状态1systemctl status firewalld 关闭SELinux1234567setenforce 0sed -i &quot;s/^SELINUX\=enforcing/SELINUX\=disabled/g&quot; /etc/selinux/configsed -i &quot;s/^SELINUX\=permissive/SELINUX\=disabled/g&quot; /etc/selinux/configsed -i &quot;s/^SELINUX\=enforcing/SELINUX\=disabled/g&quot; /etc/sysconfig/selinuxsed -i &quot;s/^SELINUX\=permissive/SELINUX\=disabled/g&quot; /etc/sysconfig/selinuxgetenforce/usr/sbin/sestatus -v 内核参数设置 编辑 1vim /etc/profile 追加 123456789101112131415161718192021222324252627282930modprobe -a br_netfilter \ ip6_udp_tunnel \ ip_set \ ip_set_hash_ip \ ip_set_hash_net \ iptable_filter \ iptable_nat \ iptable_mangle \ iptable_raw \ nf_conntrack_netlink \ nf_conntrack \ nf_conntrack_ipv4 \ nf_defrag_ipv4 \ nf_nat \ nf_nat_ipv4 \ nf_nat_masquerade_ipv4 \ udp_tunnel \ veth \ vxlan \ x_tables \ xt_addrtype \ xt_conntrack \ xt_comment \ xt_mark \ xt_multiport \ xt_nat \ xt_recent \ xt_set \ xt_statistic \ xt_tcpudp 使其生效 1source /etc/profile 查看未加载模块（什么都不显示就对了） 12345678for module in br_netfilter ip6_udp_tunnel ip_set ip_set_hash_ip ip_set_hash_net iptable_filter iptable_nat iptable_mangle iptable_raw nf_conntrack_netlink nf_conntrack nf_conntrack_ipv4 nf_defrag_ipv4 nf_nat nf_nat_ipv4 nf_nat_masquerade_ipv4 udp_tunnel veth vxlan x_tables xt_addrtype xt_conntrack xt_comment xt_mark xt_multiport xt_nat xt_recent xt_set xt_statistic xt_tcpudp;do if ! lsmod | grep -q $module; then if ! lsmod | grep -q $module /lib/modules/$(uname -r)/modules.builtin; then echo &quot;module $module is not present&quot;; fi; fi;done; 编辑 1vim /etc/sysctl.conf 追加或修改内容 1234567891011121314151617181920212223242526272829303132333435363738394041424344# 优化参数net.core.netdev_max_backlog = 262144net.core.somaxconn = 65535net.ipv4.tcp_synack_retries = 2net.ipv4.tcp_max_syn_backlog = 16384net.ipv4.tcp_syncookies = 1net.ipv4.tcp_syn_retries = 2net.ipv4.ip_local_port_range = 1024 65000net.ipv4.tcp_abort_on_overflow = 1net.ipv4.tcp_rmem = 16384 1048576 12582912net.ipv4.tcp_wmem = 16384 1048576 12582912net.ipv4.tcp_mem = 1541646 2055528 3083292net.ipv4.tcp_moderate_rcvbuf = 1net.ipv4.tcp_orphan_retries = 3net.ipv4.tcp_fin_timeout = 15net.ipv4.tcp_max_tw_buckets = 16384net.ipv4.tcp_tw_reuse = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_timestamps = 1net.ipv4.tcp_tw_recycle = 0net.ipv4.tcp_retries1 = 2net.ipv4.tcp_retries2 = 3net.ipv4.tcp_keepalive_time = 600net.ipv4.tcp_keepalive_intvl = 2net.ipv4.tcp_keepalive_probes = 3fs.file-max = 2147483647fs.nr_open = 1048576# lvs必备参数net.ipv4.conf.lo.arp_ignore = 1net.ipv4.conf.lo.arp_announce = 1net.ipv4.conf.all.arp_ignore = 1net.ipv4.conf.all.arp_announce = 1# docker必备参数net.ipv4.ip_forward = 1# kubernetes必备参数net.bridge.bridge-nf-call-iptables = 1net.bridge.bridge-nf-call-ip6tables = 1# ab参数net.nf_conntrack_max = 65535net.netfilter.nf_conntrack_tcp_timeout_established = 1200 使配置立即生效 1sysctl -p 设置文件描述符 编辑 1vim /etc/security/limits.conf 追加或修改内容 123456* soft nofile 1048576* hard nofile 1048576* soft nproc 1048576* hard nproc 1048576* soft memlock unlimited* hard memlock unlimited 编辑 1vim /etc/security/limits.d/20-nproc.conf 追加或修改内容 123456* soft nofile 1048576* hard nofile 1048576* soft nproc 1048576* hard nproc 1048576* soft memlock unlimited* hard memlock unlimited 编辑 1vim /etc/pam.d/login 追加或修改内容 1234# 32位系统session required /lib/security/pam_limits.so# 64位系统session required /lib64/security/pam_limits.so 关闭虚拟内存123swapoff -ased -i &apos;s/.*swap.*/#&amp;/&apos; /etc/fstabcat /etc/fstab 配置虚拟内存设置【可选】 物理内存小于4G（虚拟内存统一设置为4G） 1234dd if=/dev/zero of=/swap bs=1024 count=4096000mkswap /swapchmod 0600 /swapswapon /swap 物理内存小于16G（虚拟内存统一设置为8G） 1234dd if=/dev/zero of=/swap bs=1024 count=8192000mkswap /swapchmod 0600 /swapswapon /swap 物理内存小于64G（虚拟内存统一设置为16G） 1234dd if=/dev/zero of=/swap bs=1024 count=16384000mkswap /swapchmod 0600 /swapswapon /swap 物理内存小于256G（虚拟内存统一设置为32G） 1234dd if=/dev/zero of=/swap bs=1024 count=32768000mkswap /swapchmod 0600 /swapswapon /swap 设置改挂载点 1vim /etc/fstab 编辑或新增 1/swap swap swap defaults 0 0 关闭transparent_hugepage临时关闭12echo &quot;never&quot; &gt; /sys/kernel/mm/transparent_hugepage/enabledecho &quot;never&quot; &gt; /sys/kernel/mm/transparent_hugepage/defrag 永久关闭 编辑 1vim /etc/default/grub 在GRUB_CMDLINE_LINUX加入选项transparent_hugepage=never 12345# ext4GRUB_CMDLINE_LINUX=&quot;crashkernel=auto spectre_v2=retpoline rhgb quiet transparent_hugepage=never&quot;或者# xfsGRUB_CMDLINE_LINUX=&quot;crashkernel=auto spectre_v2=retpoline rd.lvm.lv=centos/root rhgb quiet transparent_hugepage=never&quot; 生成grub文件（bios） 1grub2-mkconfig -o /boot/grub2/grub.cfg 生成文件的grub文件（efi） 1grub2-mkconfig -o /boot/efi/EFI/redhat/grub.cfg 添加执行路径环境变量配置12echo &apos;export PATH=.:$PATH&apos; &gt; /etc/profile.d/here.shcat /etc/profile.d/here.sh 密码策略配置 编辑 1vim /etc/login.defs 修改 12345PASS_MAX_DAYS 99999PASS_MIN_DAYS 0=&gt;PASS_MAX_DAYS 180PASS_MIN_DAYS 7 执行 12chage --maxdays 180 rootchage --mindays 7 root 密码复杂度配置 编辑 1vim /etc/security/pwquality.conf 修改 12345# minlen = 9# minclass = 0=&gt;minlen = 8minclass = 3 密码重用限制 编辑 1vim /etc/pam.d/password-auth 修改 123password sufficient pam_unix.so sha512 shadow nullok try_first_pass use_authtok=&gt;password sufficient pam_unix.so sha512 shadow nullok try_first_pass use_authtok remember=5 编辑 1vim /etc/pam.d/system-auth 修改 123password sufficient pam_unix.so sha512 shadow nullok try_first_pass use_authtok=&gt;password sufficient pam_unix.so sha512 shadow nullok try_first_pass use_authtok remember=5 SSH限制策略 编辑 1vim /etc/ssh/sshd_config 修改 12345#ClientAliveInterval 0#ClientAliveCountMax 3=&gt;ClientAliveInterval 600ClientAliveCountMax 2 创建物理卷1pvcreate /dev/sdb 创建卷组1vgcreate storage /dev/sdb 创建逻辑卷1lvcreate -l 100%Free -n data storage 格式化逻辑卷1mkfs.xfs /dev/storage/data 创建挂载点1mkdir -p /storage/data 添加挂载信息 编辑 1vim /etc/fstab 追加 1/dev/mapper/storage-data /storage/data xfs defaults 0 0 挂载1mount -a 查看1df -hT 重启1shutdown -r now 重启后切换到root账户1su - root 开始安装Docker安装Docker12345678910yum remove docker \ docker-client \ docker-client-latest \ docker-common \ docker-latest \ docker-latest-logrotate \ docker-logrotate \ docker-selinux \ docker-engine-selinux \ docker-engine 添加Docker仓库 官方源 1yum-config-manager --add-repo https://download.docker.com/linux/centos/docker-ce.repo 阿里云源 1yum-config-manager --add-repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo 安装docker和docker-compose1yum install -y docker-ce docker-ce-cli containerd.io docker-compose 配置docker监听地址（2375按需开，因为没有密码，至少不能配置为监听公网地址）1vim /lib/systemd/system/docker.service 12# ExecStart=/usr/bin/dockerd -H fd:// --containerd=/run/containerd/containerd.sockExecStart=/usr/bin/dockerd --icc=false -H unix:///var/run/docker.sock -H tcp://127.0.0.1:2375 -H fd:// --containerd=/run/containerd/containerd.sock 或者 12sed -i &apos;s/^ExecStart=.*/ExecStart=\/usr\/bin\/dockerd --icc=false -H unix:\/\/\/var\/run\/docker.sock -H tcp:\/\/127.0.0.1:2375 -H fd:\/\/ --containerd=\/run\/containerd\/containerd.sock/&apos; /lib/systemd/system/docker.servicecat /lib/systemd/system/docker.service 配置镜像加速器&amp;私有镜像仓库的非安全访问1234567mkdir -p /etc/dockertee /etc/docker/daemon.json &lt;&lt;- &apos;EOF&apos;&#123; &quot;registry-mirrors&quot;: [&quot;https://vax5oqx1.mirror.aliyuncs.com&quot;], &quot;insecure-registries&quot;: [&quot;docker.parkson.net.cn:5000&quot;, &quot;docker-registry.parkson.net.cn:5000&quot;, &quot;docker.iparkson.net.cn:5000&quot;, &quot;docker-registry.iparkson.net.cn:5000&quot;]&#125;EOF 重新加载服务配置1systemctl daemon-reload 启用docker1systemctl enable docker 启动docker1systemctl restart docker 查看docker状态1systemctl status docker 下载convoy1wget https://github.com/rancher/convoy/releases/download/v0.5.2/convoy.tar.gz 解压1tar xvzf convoy.tar.gz 复制1\cp -f convoy/convoy convoy/convoy-pdata_tools /usr/local/bin/ 创建docker插件目录1mkdir -p /etc/docker/plugins/ 写入插件信息1echo &quot;unix:///var/run/convoy/convoy.sock&quot; &gt; /etc/docker/plugins/convoy.spec 创建convoy服务 创建卷存储目录 1mkdir -p /storage/data/docker/volumes 编辑 1vim /usr/lib/systemd/system/convoy.service 内容 12345678910111213[Unit]Description=Convoy ServiceAfter=network.target[Service]LimitNOFILE=1048576WorkingDirectory=/storage/data/docker/volumesExecStart=/usr/local/bin/convoy daemon --drivers vfs --driver-opts vfs.path=/storage/data/docker/volumesRestart=on-failureRestartSec=5[Install]WantedBy=multi-user.target 启动服务 启用服务 1systemctl enable convoy 启动服务 1systemctl start convoy 查看状态 1systemctl status convoy 重启docker服务1systemctl restart docker 编辑crontab12345mkdir -p /etc/cron.d/tee /etc/cron.d/docker &lt;&lt;- &apos;EOF&apos;0 0 * * * root docker images | awk &apos;$1 == &quot;&lt;none&gt;&quot; || $2 == &quot;&lt;none&gt;&quot; &#123;print $3&#125;&apos; | xargs -r -t docker rmi* * * * * root docker ps -a | grep &apos;(unhealthy)&apos; | awk &apos;&#123;print $1&#125;&apos; | xargs -r -t docker restartEOF 启用crond1systemctl enable crond 启动crond1systemctl restart crond 查看crond状态1systemctl status crond 查看cron日志1cat /var/log/cron 退出root账户1exit 将当前用户加入到docker组1sudo gpasswd -a $&#123;USER&#125; docker 1sudo newgrp docker 配置DOCKER_HOST环境变量【可选】1vim ~/.bashrc 追加 1export DOCKER_CONTENT_TRUST=1 审核Docker文件和目录 编辑 1vim /etc/audit/audit.rules 追加 123456-w /var/lib/docker -k docker-w /etc/docker -k docker-w /usr/lib/systemd/system/docker.service -k docker-w /usr/lib/systemd/system/docker.socket -k docker-w /usr/bin/docker-containerd -k docker-w /usr/bin/docker-runc -k docker 编辑 1vim /etc/audit/rules.d/audit.rules 追加 123456-w /var/lib/docker -k docker-w /etc/docker -k docker-w /usr/lib/systemd/system/docker.service -k docker-w /usr/lib/systemd/system/docker.socket -k docker-w /usr/bin/docker-containerd -k docker-w /usr/bin/docker-runc -k docker 启用 1systemctl enable auditd 启动 1systemctl restart auditd 查看 1systemctl enable auditd]]></content>
      <categories>
        <category>docker</category>
      </categories>
      <tags>
        <tag>docker</tag>
        <tag>basic</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markowitz's Mean-Variance Model Derivation in Python]]></title>
    <url>%2Fblog%2F2020%2F01%2F17%2FMarkowitz-s-Mean-Variance-Model-Derivation-in-Python%2F</url>
    <content type="text"><![CDATA[Markowitz’s Mean-Variance Model Derivation in PythonWritten by Jiang Rongrong(2019E8010663001) Interested by “Lecture 3.Quadratic Programing and Portfolio Selection Theory”,I’ve consulted a number of books about Markowitz’s Mean-Variance Model.Therefore,I want to make some discussion about what I’ve learnt and the expeirments when I tried to figure out this model. Theory SummaryMarkowitz made the following assumptions while developing the Mean-Variance Model: Risk of a portfolio is based on the variability of returns from the said portfolio. An investor is risk averse. An investor prefers to increase consumption. The investor’s utility function is concave and increasing, due to his risk aversion and consumption preference. Analysis is based on single period model of investment. An investor either maximizes his portfolio return for a given level of risk or maximizes his return for the minimum risk. An investor is rational in nature. To choose the best portfolio from a number of possible portfolios, each with different return and risk, two separate decisions are to be made, detailed in the below sections: Determination of a set of efficient portfolios. Selection of the best portfolio out of the efficient set. Above quotes from wiki Based on above assumptions and thoughts,Markowitz establish his Mean-Variance Model as follows: Formula: Constraints: notes:xi stands for the weights of asset i,ri stands for the return of asset i SimulationsIn this part ,I will try to use random data to simulate the derivation process of Mean-Variance Model.Thanks for my boyfriend’s leading,I choose python as the main tool.And each line of code is with notes if necessary. First,initiate and import necessary util packages. Intro of each util package: numpy and pandas: Matrix calculate matplotlib : Data plot cvxopt : A convex solver 12345678910import numpy as npimport matplotlib.pyplot as pltimport cvxopt as optfrom cvxopt import blas, solversimport pandas as pdnp.random.seed(123)# Turn off progress printing solvers.options['show_progress'] = False Assume that there are 4 assets, each with a return series of length 1000. The func numpy.random.randn is used to sample returns from a normal distribution.12345678## NUMBER OF ASSETSn_assets = 4## NUMBER OF OBSERVATIONSn_obs = 1000return_vec = np.random.randn(n_assets, n_obs) Plot the return series of the assumed 4 assets.123plt.plot(return_vec.T, alpha=.4);plt.xlabel('time')plt.ylabel('returns'); These return series can be used to create a wide range of portfolios. After that random weight vectors and plot those portfolios will be produced. As I want all my capital to be invested, the weights will have to sum to one.1234567def rand_weights(n): ''' Produces n random weights that sum to 1 ''' k = np.random.rand(n) return k / sum(k)print(rand_weights(n_assets))print(rand_weights(n_assets)) Next, evaluate how these random portfolios would perform by calculating the mean returns and the volatility (here using standard deviation). I set a filter so that only portfolios with a standard deviation of &lt; 2 are ploted for better illustration. 12345678910111213141516def random_portfolio(returns): ''' Returns the mean and standard deviation of returns for a random portfolio ''' p = np.asmatrix(np.mean(returns, axis=1)) w = np.asmatrix(rand_weights(returns.shape[0])) C = np.asmatrix(np.cov(returns)) mu = w * p.T sigma = np.sqrt(w * C * w.T) # This recursion reduces outliers to keep plots pretty if sigma &gt; 2: return random_portfolio(returns) return mu, sigma Calculate the return using R=pTw where R is the expected return, pT is the transpose of the vector for the mean returns for each time series and w is the weight vector of the portfolio. p is a N*1 column vector, so pT turns is a 1*N row vector which can be multiplied with the weight (column) vector w to give a scalar result. Next, Calculate the standard deviation sigma=sqrt(wTCw) where C is the N*N covariance matrix of the returns. Generate the mean returns and volatility for 500 random portfolios and plot them:12345678910n_portfolios = 500means, stds = np.column_stack([ random_portfolio(return_vec) for _ in range(n_portfolios)])plt.plot(stds, means, 'o', markersize=5)plt.xlabel('std')plt.ylabel('mean')plt.title('Mean and standard deviation of returns of randomly generated portfolios'); By observing the picture it can be figured out that they form a characteristic parabolic shape called the “Markowitz Bullet” whose upper boundary is called the “efficient frontier”, where investors can have the lowest variance for a given expected return. Now the efficient frontier in Markowitz-style can be calculated. This is done by minimizing wTCw for fixed expected portfolio return RTw while keeping the sum of all the weights equal to 1: sum(wi)=1 (i=1,2,3…n) Here I parametrically run through RTw=miu and find the minimum variance for different miu‘s. 12345678910111213141516171819202122232425262728293031323334353637def optimal_portfolio(returns): n = len(returns) returns = np.asmatrix(returns) N = 100 mus = [10**(5.0 * t/N - 1.0) for t in range(N)] # Convert to cvxopt matrices S = opt.matrix(np.cov(returns)) pbar = opt.matrix(np.mean(returns, axis=1)) # Create constraint matrices G = -opt.matrix(np.eye(n)) # negative n x n identity matrix h = opt.matrix(0.0, (n ,1)) A = opt.matrix(1.0, (1, n)) b = opt.matrix(1.0) # Calculate efficient frontier weights using quadratic programming portfolios = [solvers.qp(mu*S, -pbar, G, h, A, b)['x'] for mu in mus] ## CALCULATE RISKS AND RETURNS FOR FRONTIER returns = [blas.dot(pbar, x) for x in portfolios] risks = [np.sqrt(blas.dot(x, S*x)) for x in portfolios] ## CALCULATE THE 2ND DEGREE POLYNOMIAL OF THE FRONTIER CURVE m1 = np.polyfit(returns, risks, 2) x1 = np.sqrt(m1[2] / m1[0]) # CALCULATE THE OPTIMAL PORTFOLIO wt = solvers.qp(opt.matrix(x1 * S), -pbar, G, h, A, b)['x'] return np.asarray(wt), returns, risksweights, returns, risks = optimal_portfolio(return_vec)plt.plot(stds, means, 'o')plt.ylabel('mean')plt.xlabel('std')plt.plot(risks, returns, 'y-o')print(weights) In yellow is the optimal portfolios for each of the desired returns (i.e. the mus). In addition, the weights for one optimal portfolio are also calculated.]]></content>
      <categories>
        <category>项目</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[码农正确装逼姿势——书法篇]]></title>
    <url>%2Fblog%2F2019%2F09%2F13%2F%E7%A0%81%E5%86%9C%E6%AD%A3%E7%A1%AE%E8%A3%85%E9%80%BC%E5%A7%BF%E5%8A%BF%E2%80%94%E2%80%94%E4%B9%A6%E6%B3%95%E7%AF%87%2F</url>
    <content type="text"><![CDATA[前言 可怜今夕月，向何处，去悠悠？是别有人间，那边才见，光影东头？是天外。空汗漫，但长风浩浩送中秋？飞镜无根谁系？姮娥不嫁谁留？——宋·辛弃疾 中秋佳节，花好月圆，行书一幅赠与（zhuang）友人(bi)。 我一个程序员，写得了代码不说，还能写一手好字，这么多才多艺的程序员到哪里去找！？ 所以这显然是装出来的，我们这类人最擅长的就是利用代码来解决问题，这书法当然是用代码写的。 正文先放个源码链接，各位可以直接clone然后稍作修改用起来，但记得star一下啊！！ 程序完全用python极其标准库turtle写的，先简单介绍一下turtle，如果各位小学有计算机启蒙类课程的话可能还记得这只小海龟，它其实是一个很基础的图形绘制库，主要用来教育小孩进行编程入门学习。他在各个语言内都有实现，而我们今天用的就是它在python语言下的实现。 具体的api各位可以参考官方文档，毕竟给小孩入门编程用的，非常的简单易懂。 实现此程序需要有以下两个功能： 写正文与落款：落款其实和正文相同，只是字体大小不一样罢了，因此用同一个函数实现； 盖章：因为盖章又分为阴刻和阳刻，具体实现起来还是有些去别的，因此两种盖章方式用两个函数实现。 实现写正文与落款的函数：12345from turtle import *def write_in_arts(mystr, font='米芾书法字体', fontsize=60): pendown() write(mystr, font=(font, fontsize, 'normal')) penup() 关于字体，各位可以去网上搜索下载安装（很无脑就不谈了），也可以直接用repo里附带的两个字体 实现阳刻的函数：123456789def garland(): # 阳刻 pensize(2) pencolor('red') pendown() for i in range(4): # 爱的魔力转圈圈，其实就是弄个方形然后往里面写字，边框和字体都是红色，背景为白色 forward(56) left(90) write('雨散\n花人', font=('经典繁方篆', 20, 'normal')) # 刻上您的诨号的时候要注意印章文字的顺序，比如我的雨花散人（居住在雨花街道的咸鱼）就要像这样写才对 penup() 实现阴刻的函数：123456789101112def diaglyph(): # 阴刻 pensize(3) # 为了美观，将字体略微设置的大一些 pencolor('white') fillcolor('red') # 设置填充色为红色 begin_fill() # 对接下来被框起来的部分进行填充 for i in range(4): # 继续爱的魔力转圈圈 forward(56) left(90) end_fill() # 填充完毕 write('雨散\n花人', font=('经典繁方篆', 20, 'normal')) penup() 最后，把上面的函数组装起来，机器人变形出发！！123456789101112def zhuangbi(): penup() goto(-60, -100) # 文字位置什么的，各位根据需要自己适当调整 write_in_arts('装\n逼', fontsize=90) goto(-250, 0) write_in_arts('大\n帅\n逼', fontsize=30) goto(-270, -70) garland() goto(-270, -140) diaglyph() hideturtle() done() 效果如下： Todo这个程序还有一些缺陷，但是蛮有意思的，简单搜了一下没有发现同类的小程序，所以计划完善一下给他发布成一个微信小程序（为什么又在立flag呢），因此列一下待改善和开发的点： 添加交互，将其发布成微信小程序； 正文的文字排版不够智能，排版应该能根据输入自动优化； 正文和刻章的字体最好能够提供字体列表进行选择； 刻章的文字顺序需要手工调整，改为自动根据输入生成； 目前只能刻正方形的章，改为可以支持圆形的章； 最后生成的结果通过程序只能保存为eps格式，应当改为能够支持生成多种格式；]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>项目</tag>
        <tag>小工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[一个小工具的开发日记]]></title>
    <url>%2Fblog%2F2019%2F08%2F28%2F%E4%B8%80%E4%B8%AA%E5%B0%8F%E5%B7%A5%E5%85%B7%E7%9A%84%E5%BC%80%E5%8F%91%E6%97%A5%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[前言 7月4日新开这本日记，也为了督促自己下个学期多下些苦功。先要读完手边的莎士比亚的《亨利八世》。7月13日打牌。7月14日打牌。7月15日打牌。7月16日胡适之啊胡适之！你怎么能如此堕落！先前订下的学习计划你都忘了吗？子曰：“吾日三省吾身。”不能再这样下去了！7月17日打牌。7月18日打牌。 以上摘自胡适先生的留学日记，明明是别人的故事，主人公却好像是自己。 于是抽空就有了此文，虽然还是咸鱼，但至少死鱼眼里泛出些许光彩了。 正文闲话不提，直接进入正文。故事的起因就是最近常跑人资办公室的我被人事姐姐们拜托帮忙做一个小工具，希望能够在培训会上用来随机抽问题然后找人回答，然后希望这个小工具能够尽快做好给她们。 我寻思这不挺简单么，也就顺口答应了下来。事后证明…… 真的很简单，不然本咸鱼为什么用这个故事来水文呢！？ 事情虽然简单，该走的流程还是要走的，我们按照软件开发流程一步步来。 需求分析一番沟通，确认了人事姐姐们的主要需求如下： 随机抽号：可以设置抽取数字范围和抽取数字个数，点开始按钮数字开始滚动，点结束按钮出现抽号结果 背景与标题设置：可以设置抽号程序的背景界面与大标题，让抽号程序显示内容和会议更加贴合 设计考虑开发速度和复杂度，这个小工具简单地用python开发就完事了，毕竟人生苦短，我用python。 技术选型 开发语言：python 程序架构：cs架构（一个自带GUI的小工具而已，没必要搞bs架构） 主要python工具包：tkinter(标准库)，Pillow(图像处理库) 界面设计灵魂画师上线！ 开发又到了我们最爱的coding环节，请各位打开浏览器，面向搜索引擎（最好是Google）开始编写代码。 具体的我们就不讲了，源码都在这，tkinter不会用请看官方文档，就主要说一下一些有趣的细节。 如何设置一个弹出窗口用于接收参数设置：其实就是编写一个主窗口类（继承tk.TK类），一个弹出窗口类（继承tk.Toplevel类）。大致的流程是这样的： 创建主窗口实例； 触发绑定在主窗口实例上的触发式组件； 创建弹出窗口实例； 通过弹出窗口会话收集数据； 主窗口实例等待弹出窗口会话销毁； 主窗口实例获取弹出窗口实例收集到的数据。 sample:123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384import tkinter as tk# 主窗class MainWindow(tk.Tk): def __init__(self): super().__init__() self.title('主窗口') # 程序参数/数据 self.name = tk.StringVar() self.name.set('张三') self.age = tk.IntVar() self.age.set(30) # 设置主窗口界面 self.setupUI() def setupUI(self): row1 = tk.Frame(self) row1.pack(fill="x") tk.Label(row1, text='姓名：', width=8).pack(side=tk.LEFT) self.l1 = tk.Label(row1, textvariable=self.name, width=20) self.l1.pack(side=tk.LEFT) row2 = tk.Frame(self) row2.pack(fill="x") tk.Label(row2, text='年龄：', width=8).pack(side=tk.LEFT) self.l2 = tk.Label(row2, textvariable=self.age, width=20) self.l2.pack(side=tk.LEFT) row3 = tk.Frame(self) row3.pack(fill="x") tk.Button(row3, text="设置", command=self.setup_config).pack(side=tk.RIGHT) # 通过Button触发 # 设置参数 def setup_config(self): # 接收弹窗的数据 res = self.ask_userinfo() if res is None: return # 更改参数，更新界面 self.name.set(res[0]) self.age.set(res[1]) # 弹窗 def ask_userinfo(self): popup_window = PopupWindow() # 创建弹窗实例 self.wait_window(popup_window) # 等待弹窗会话关闭 return popup_window.userinfo # 获取绑定在弹窗实例私有变量上的数据# 弹窗class PopupWindow(tk.Toplevel): def __init__(self): super().__init__() self.title('弹出窗口') # 设置弹窗界面 self.setup_UI() def setup_UI(self): row1 = tk.Frame(self) row1.pack(fill="x") tk.Label(row1, text='姓名：', width=8).pack(side=tk.LEFT) self.name = tk.StringVar() tk.Entry(row1, textvariable=self.name, width=20).pack(side=tk.LEFT) row2 = tk.Frame(self) row2.pack(fill="x", ipadx=1, ipady=1) tk.Label(row2, text='年龄：', width=8).pack(side=tk.LEFT) self.age = tk.IntVar() tk.Entry(row2, textvariable=self.age, width=20).pack(side=tk.LEFT) row3 = tk.Frame(self) row3.pack(fill="x") tk.Button(row3, text="确定", command=self.save).pack(side=tk.LEFT) tk.Button(row3, text="取消", command=self.cancel).pack(side=tk.LEFT) def save(self): self.userinfo = [self.name.get(), self.age.get()] # 将用户输入的数据绑定到实例私有变量上 self.destroy() # 销毁窗口 def cancel(self): self.userinfo = None self.destroy()if __name__ == '__main__': app = MainWindow() app.mainloop() 如何设置程序背景图片tkinter在界面美化这方面其实并不是很好用，在设置程序图片这一块遇上了不少坑，权且记录一下： 无法直接对tk.TK或者tk.Frame这类容器设置背景图片，最多设置背景色（至少我没找到什么好办法），解决方法就是搞一个tk.Label平铺在整个tk.TK上，然后通过tk.Label的image属性设置背景图片； 在按照上述方法设置背景图片的过程中发现图片并未平铺到整个tk.Label,需要手动设置图片大小；且需要预先使用PIL对图片进行处理，否则某些图片格式tkinter不支持； sample:1234567891011import tkinter as tkfrom PIL import ImageTk,Imageroot=tk.Tk(bg='blue') # 设置一个背景色方便看出问题width,height=root.maxsize() # 获取屏幕大小root.geometry('&#123;&#125;x&#123;&#125;'.format(width,height)) # 最大化程序窗口image=Image.open('1.jpg')image=image.resize((width,height),Image.ANTIALIAS) # 不手动调整图片尺寸的话会发现图片不会平铺到整个tk.Label上bg_image=ImageTk.PhotoImage(image) # 利用PIL形成转换管道的方式才能正确读取jpg图片l=tk.Label(root,bg='white',image=bg_image) l.pack(expand=True,fill=tk.BOTH) # 确认label已经平铺到整个程序窗口 测试与发布 测试：xjbd一遍，OK了； 发布：利用pyinstaller将程序打包成exe即可。 总结我对自己说：“张杰啊张杰！你怎么能如此堕落！先前立的flag你都忘了吗？子曰：‘吾日三省吾身。’不能再这样下去了！”]]></content>
      <categories>
        <category>项目</category>
      </categories>
      <tags>
        <tag>项目</tag>
        <tag>小工具</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python and Selenium]]></title>
    <url>%2Fblog%2F2019%2F07%2F05%2FPython-and-Selenium%2F</url>
    <content type="text"><![CDATA[What’s Selenium Selenium automates browsers. That’s it! What you do with that power is entirely up to you. Primarily, it is for automating web applications for testing purposes, but is certainly not limited to just that. Boring web-based administration tasks can (and should!) be automated as well. Selenium has the support of some of the largest browser vendors who have taken (or are taking) steps to make Selenium a native part of their browser. It is also the core technology in countless other browser automation tools, APIs and frameworks. 以上摘自Selenium官方首页 一句话概括：Selenium可以自动控制浏览器，一般用于自动化测试，但是你想用它做其他的事情（比如爬虫、比如代替做一些无聊重复的基于网站的工作）当然也是OK的，总之根据你的需要自己鼓捣去吧。 Why Selenium目前接触过的同类自动化测试工具有3个： Selenium Puppeteer Splash 他们各有优缺点，根据需要使用，个人的话更偏向于使用Selenium: 工具 官方文档 社区活跃度 编程语言 支持浏览器 安装 API BUG 效率 Selenium 完备易读 非常活跃，问题通常能得到解答 Java,Python,PHP 基本支持所有主流浏览器 较为简单快捷，selenium加上相应浏览器与浏览器驱动即可 丰富且易于使用 较少 阻塞式，最慢，最好利用官方提供的selenium grid集群提升效率 Puppeteer 完备易读 相对活跃 Javascript,另外虽然有个人开发的pyppeteer作为python支持，但是不建议使用 本质其实是一个无头的chrome浏览器 基于node.js，需要使用npm安装，相对复杂 相对丰富但因为加入了协程机制使用起来较为复杂 目前仍然有不少坑 由于使用协程机制，速度较快 Splash 完备易读 相对活跃 Lua，对接scrapy时非常好用 本质是一个异步js渲染服务引擎 非常简单，使用docker安装 丰富，但使用起来时需要编写lua脚本作为请求参数，略微复杂 有一些但不多，可以接受 异步，配合协程机制（比如scrapy）很快 Howto be continued…]]></content>
      <categories>
        <category>爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>自动化</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Markdown基本语法]]></title>
    <url>%2Fblog%2F2019%2F03%2F28%2FA-Test-Post%2F</url>
    <content type="text"><![CDATA[记录一下常用的markdown语法，just for convenience! 字体 语法 结果 *斜体* 斜体 **加粗** 加粗 ~~删除线~~ 删除线 字体下&lt;sub&gt;logo&lt;/sub&gt;标 字体下logo标 字体上&lt;sup&gt;logo&lt;/sup&gt;标 字体上logo标 &lt;u&gt;下划线&lt;/u&gt; 下划线 格式控制与转义字符 换行：使用连续两个回车或者&lt;br&gt;标签 缩进：使用转义字符中的空格 对齐：使用&lt;p align=”left/center/right”&gt;&lt;/p&gt;控制 转义：一般情况下使用\+需要转义的字符进行转义，部分字符需要使用下列转义字符表 转义字符 效果 解释 &amp;nbsp; &nbsp;缩进 不换行空格 &amp;ensp; &ensp;缩进 半角空格 &amp;emsp; &emsp;缩进 全角空格 &amp; &amp; 与号 &amp;lt; &lt; 小于符号 &amp;gt; &gt; 大于符号 标题1到6个#号表示从大到小6个级别的标题 分割线空行中三个以上的*号或者减号或者下划线为一个分割线，例如： 注释语法： &lt;!–注释–&gt; 效果： 注释不会被显示。 代码块行内代码语法： 这句`print(Hello world!)`是行内代码 效果： 这句print(Hello world!)是行内代码 代码框语法： ```bash ls -a ``` 效果：1ls -a 引用语法: > 引用1 > > 引用2 > > > 引用3 效果: 引用1 引用2 引用3 备注:引用作为一个区块，可以在区块内部嵌套使用列表、代码块、标题等，例如： 引用1（标题） 引用2（列表） 1引用3（代码块） 列表无序列表语法：+ list1 + list2 + list3 效果: list1 list2 list3 有序列表语法： 1. list1 2. list2 3. list3 效果: list1 list2 list3 备注:有序列表的序号是根据第一个列表项的序号自增的，比如第一个列表项的序号为3，那么不管之后的列表项的序号是多少，都会是4，5，6…… 表格语法： col1|col2|col3 :-|:-|:- dt|dt|dt dt|dt|dt dt|dt|dt 效果： col1 col2 col3 dt dt dt dt dt dt dt dt dt 备注：通过在第二行中调整冒号与横线实现表格对齐，具体如下： :- （左对齐） :-:（居中） -:（右对齐） 链接语法： [链接文字](链接地址) [链接文字](链接地址 “链接说明”) 效果： 链接文字 链接文字 图片语法： ![链接图片](链接地址) 效果：]]></content>
      <categories>
        <category>markdown</category>
      </categories>
      <tags>
        <tag>markdown</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2Fblog%2F2019%2F03%2F28%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
